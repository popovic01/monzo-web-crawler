# Web Crawler for the Monzo Home Task üíöüíõüß°

### Overview
This is a simple web crawler implemented in Python. Starting from a given URL, it visits all pages **within the same domain**.  

For each visited page, it prints the URL and all the **internal links** found.  

The crawler avoids external domains (e.g., Facebook, Twitter) and limits itself to a single subdomain.

---

## üêç Language Choice: Python
- Python offers excellent libraries for web crawling and HTML parsing (`requests`, `BeautifulSoup`).
- Concurrency is simple to implement and suitable for I/O-bound tasks like network requests.
- Python‚Äôs readability allows for a clean, modular structure, making debugging and reasoning about architecture easier.

---

## How It Works
1. Start with the **initial URL** provided by the user.
2. Fetch and parse the page using `requests` and `BeautifulSoup`.
3. Extract all **valid internal links**.
4. Add unvisited links to a **queue**.
5. Add the processed URL to a **set of visited URLs**.
6. Continue until there are no more URLs to visit.

---

## üß© Core Components
- **crawler.py** ‚Äì Core crawling logic
  - Handles exceptions (timeouts, bad URLs) to avoid crashes.
- **utils.py** ‚Äì Utility functions
  - URL normalization (remove fragments, lowercase domains, strip trailing slashes).
  - Valid URL checks and internal link extraction.
- **main.py** ‚Äì Entry point
  - Accepts the starting URL.
  - Initializes the crawler and starts crawling.

---

## üß© Data Structures
1. **Queue** (`deque`) ‚Äî Pending URLs
   - Stores URLs to visit next (FIFO order).
   - O(1) append/pop operations.
2. **Set** ‚Äî Visited URLs
   - Fast O(1) lookups to prevent revisiting and infinite loops.
3. **List** ‚Äî Links found per page
   - Stored temporarily when parsing each page for printing.
---

## How to Run

### 1. Install Python (if not already installed)

### 2. Clone the repository and navigate to the project directory.
`git clone <repo_url>`
`cd <repo_folder>`

### 3. Install dependencies
pip install -r requirements.txt

### 4. Run the crawler
python main.py

## ‚ö° Future improvements
- Add async crawler version using aiohttp for greater performance.
- Export results for sitemap visualization.
- Request caching (e.g. store already crawled initial URLs in a file, for a few days).
- Error handling could include retry mechanisms.
- Differentiate http and https URLs. ???
- Respect *robots.txt* if you want to show awareness of real-world crawler ethics (politeness).
